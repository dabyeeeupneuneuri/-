{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras\n",
    "import numpy as np\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.initializers import Constant\n",
    "from keras import optimizers\n",
    "\n",
    "sentences=[]\n",
    "x=0\n",
    "with open('canonical_smiles.txt', 'r') as smile:\n",
    "    for line in smile:\n",
    "        if x<=600000:\n",
    "            sentences.append(line[:-1])\n",
    "            x+=1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "word_sequence = list(\"\".join(sentences))\n",
    "word_list = list(set(list(\"\".join(sentences))))\n",
    "word_dict = {w: i for i, w in enumerate(word_list)}\n",
    "\n",
    "skip_grams = []\n",
    "\n",
    "for i in range(1, len(word_sequence) - 1):\n",
    "    target = word_dict[word_sequence[i]]\n",
    "    context = [word_dict[word_sequence[i - 1]], word_dict[word_sequence[i + 1]]]\n",
    "    for w in context:\n",
    "        skip_grams.append([target, w])\n",
    "        \n",
    "def random_batch(data, size):\n",
    "    random_inputs = []\n",
    "    random_labels = []\n",
    "    random_index = np.random.choice(range(len(data)), size, replace=False)\n",
    "\n",
    "    for i in random_index:\n",
    "        random_inputs.append(data[i][0])  # target\n",
    "        random_labels.append([data[i][1]])  # context word\n",
    "\n",
    "    return random_inputs, random_labels\n",
    "\n",
    "training_epoch = 300\n",
    "learning_rate = 0.001\n",
    "batch_size = 20\n",
    "embedding_size = 128\n",
    "num_sampled = 15\n",
    "voc_size = len(word_list)\n",
    "\n",
    "inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "\n",
    "embeddings = tf.Variable(tf.random_uniform([voc_size, embedding_size], -1.0, 1.0))\n",
    "selected_embed = tf.nn.embedding_lookup(embeddings, inputs)\n",
    "\n",
    "nce_weights = tf.Variable(tf.random_uniform([voc_size, embedding_size], -1.0, 1.0))\n",
    "nce_biases = tf.Variable(tf.zeros([voc_size]))\n",
    "\n",
    "loss = tf.reduce_mean(\n",
    "            tf.nn.nce_loss(nce_weights, nce_biases, labels, selected_embed, num_sampled, voc_size))\n",
    "\n",
    "train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    for step in range(1, training_epoch + 1):\n",
    "        batch_inputs, batch_labels = random_batch(skip_grams, batch_size)\n",
    "\n",
    "        _, loss_val = sess.run([train_op, loss],\n",
    "                               feed_dict={inputs: batch_inputs,\n",
    "                                          labels: batch_labels})\n",
    "        \n",
    "        if step % 10 == 0:\n",
    "            print(\"loss at step \", step, \": \", loss_val)\n",
    "    \n",
    "    trained_embeddings = embeddings.eval()\n",
    "    \n",
    "dix={}\n",
    "with open(\"dic.txt\", \"r\") as dic:\n",
    "    for index, line in enumerate(dic.readlines()):\n",
    "        dix[line[0]]=index\n",
    "    \n",
    "def smilecode(code):\n",
    "    chlist=list(code)\n",
    "    reslist=[]\n",
    "    for item in chlist:\n",
    "        reslist.append(dix[item])\n",
    "    return reslist\n",
    "\n",
    "temp_x=[]\n",
    "temp_y=[]\n",
    "\n",
    "with open(\"new 1.txt\", \"r\") as toxic:\n",
    "    for line in toxic:\n",
    "        linlist=line.split(\"\\t\")\n",
    "        temp_x.append(smilecode(linlist[0]))\n",
    "        temp_y.append(int(linlist[5]))\n",
    "\n",
    "from random import shuffle\n",
    "\n",
    "ln = len(temp_x)\n",
    "ind=[i for i in range(ln)]\n",
    "shuffle(ind)\n",
    "\n",
    "for i in ind:\n",
    "    temp_x.append(temp_x[i])\n",
    "    temp_y.append(temp_y[i])\n",
    "temp_x=temp_x[ln:]\n",
    "temp_y=temp_y[ln:]\n",
    "\n",
    "n=int(ln*0.2)\n",
    "\n",
    "train_data=temp_x[n:]\n",
    "test_data=temp_x[:n]\n",
    "train_labels=temp_y[n:]\n",
    "test_labels=temp_y[:n]\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=56):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence]=1.\n",
    "    return results\n",
    "\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)\n",
    "\n",
    "y_train = np.asarray(train_labels).astype(\"float32\")\n",
    "y_test = np.asarray(test_labels).astype(\"float32\")\n",
    "\n",
    "# Embedding\n",
    "max_features = 51\n",
    "maxlen = 56\n",
    "embedding_size = 128\n",
    "\n",
    "# Convolution\n",
    "kernel_size = 5\n",
    "filters = 64\n",
    "pool_size = 4\n",
    "\n",
    "# LSTM\n",
    "lstm_output_size = 128\n",
    "\n",
    "for r in range(0.0005, 0.005, 0.0005):\n",
    "    for dec in range(0.005, 0.05, 0.005):\n",
    "        for i in range(5):\n",
    "            model = models.Sequential()\n",
    "\n",
    "            model.add(layers.Embedding(max_features,\n",
    "                                       embedding_size,\n",
    "                                       input_length=maxlen,\n",
    "                                       embeddings_initializer=Constant(trained_embeddings),\n",
    "                                       trainable=False))\n",
    "\n",
    "            model.add(layers.Dropout(0.3))\n",
    "            model.add(layers.Conv1D(filters,\n",
    "                             kernel_size,\n",
    "                             padding='valid',\n",
    "                             activation='relu'))\n",
    "            model.add(layers.MaxPooling1D(pool_size=pool_size))\n",
    "            model.add(layers.Bidirectional(layers.GRU(lstm_output_size)))\n",
    "            model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "            model.compile(optimizer=optimizers.Adam(lr=r, decay=dec),\n",
    "                          loss='binary_crossentropy',\n",
    "                          metrics=['accuracy'])\n",
    "\n",
    "            history = model.fit(x_train,\n",
    "                                y_train,\n",
    "                                epochs=800,\n",
    "                                validation_data=(x_test, y_test))\n",
    "\n",
    "            history_dict = history.history\n",
    "            acc = history_dict['acc']\n",
    "            val_acc = history_dict['val_acc']\n",
    "            \n",
    "            accstr=\" [ \"\n",
    "            for item in val_acc:\n",
    "                accstr=accstr+item\n",
    "            accstr=accstr+\" ]\\n\"\n",
    "            \n",
    "            with open(\"acclog.txt\", \"a+\") as acclog:\n",
    "                acclog.write(\"lr=\"+r+\" decay=\"+dec+\" maxacc=\"+np.amax(val_acc)+accstr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
